{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07678f4f-13ae-4316-9a5a-473262b6c159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import delta\n",
    "import pyspark\n",
    "from delta import configure_spark_with_delta_pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47bfc5d2-a5d1-4a49-9930-057d1a89c01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = (\n",
    "    pyspark.sql.SparkSession.builder.appName(\"MyApp\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d61f9049-e3d4-488b-b216-92586144752a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/matthew.powers/opt/miniconda3/envs/pyspark-330-delta-210/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/matthew.powers/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/matthew.powers/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-07890284-7c57-4d1b-9052-784fa5c2d13d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.1.0 in central\n",
      "\tfound io.delta#delta-storage;2.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      ":: resolution report :: resolve 321ms :: artifacts dl 13ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.1.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-07890284-7c57-4d1b-9052-784fa5c2d13d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/16ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/13 10:02:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e4ce91-1f50-497e-9af1-87c00c1b3f96",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cdaf779-4e68-4ad7-950b-cc789106604e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f99d811e-aa89-41aa-85a7-33426d7b0402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import quinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b31e4350-d796-4430-a12b-7973555cca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.range(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "504a493d-20b2-46aa-a03f-09a7b8896a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                        (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c95158ff-a250-4bde-acb6-434cb79e55c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "quinn.validate_absence_of_columns(df, [\"age\", \"cool\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd639668-e751-4b54-b666-6b5eacc3a6f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "DataFrameProhibitedColumnError",
     "evalue": "The ['id'] columns are not allowed to be included in the DataFrame with the following columns ['id']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataFrameProhibitedColumnError\u001b[0m            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mquinn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_absence_of_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/pyspark-330-delta-210/lib/python3.9/site-packages/quinn/dataframe_validator.py:43\u001b[0m, in \u001b[0;36mvalidate_absence_of_columns\u001b[0;34m(df, prohibited_col_names)\u001b[0m\n\u001b[1;32m     38\u001b[0m error_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{extra_col_names}\u001b[39;00m\u001b[38;5;124m columns are not allowed to be included in the DataFrame with the following columns \u001b[39m\u001b[38;5;132;01m{all_col_names}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     39\u001b[0m     extra_col_names\u001b[38;5;241m=\u001b[39mextra_col_names,\n\u001b[1;32m     40\u001b[0m     all_col_names\u001b[38;5;241m=\u001b[39mall_col_names\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_col_names:\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DataFrameProhibitedColumnError(error_message)\n",
      "\u001b[0;31mDataFrameProhibitedColumnError\u001b[0m: The ['id'] columns are not allowed to be included in the DataFrame with the following columns ['id']"
     ]
    }
   ],
   "source": [
    "quinn.validate_absence_of_columns(df, [\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe399fc5-7b33-439a-9540-25884fead3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from quinn import validate_absence_of_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94365d4-9873-4c1b-9302-dc4fb7d5c63f",
   "metadata": {},
   "source": [
    "validate_absence_of_columns(df, [\"age\", \"cool\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162a630f-478a-4a2f-8986-2a79f8ca29ff",
   "metadata": {},
   "source": [
    "## Column functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6846cf51-8a28-4902-855e-0bc75a3f0536",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(\"karen\", 56), (\"jodie\", 16), (\"jason\", 3)]).toDF(\n",
    "    \"first_name\", \"age\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77549b0b-db21-42d6-acf6-a76e7ae04448",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|first_name|age|\n",
      "+----------+---+\n",
      "|     karen| 56|\n",
      "|     jodie| 16|\n",
      "|     jason|  3|\n",
      "+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c30c77be-cd99-476c-8f95-88ba49930bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93b683c4-f103-446c-ae05-729e10370ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def life_stage(col):\n",
    "    return (\n",
    "        F.when(col < 13, \"child\")\n",
    "        .when(col.between(13, 18), \"teenager\")\n",
    "        .when(col > 18, \"adult\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f6f9b62-ae9e-47f2-9f92-6c392b3eb0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----------+\n",
      "|first_name|age|life_stage|\n",
      "+----------+---+----------+\n",
      "|     karen| 56|     adult|\n",
      "|     jodie| 16|  teenager|\n",
      "|     jason|  3|     child|\n",
      "+----------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"life_stage\", life_stage(F.col(\"age\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb9d120-8923-4295-94d0-134aaf0ec39f",
   "metadata": {},
   "source": [
    "## Schema Dependent Custom DataFrame Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b46f7cd7-c3b3-4c68-ad59-e497383b4905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_life_stage(df):\n",
    "    return df.withColumn(\"life_stage\", life_stage(F.col(\"age\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a01b1ac-e19f-4a20-9053-12040f8131b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----------+\n",
      "|first_name|age|life_stage|\n",
      "+----------+---+----------+\n",
      "|     karen| 56|     adult|\n",
      "|     jodie| 16|  teenager|\n",
      "|     jason|  3|     child|\n",
      "+----------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.transform(with_life_stage).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f51c218-4b2a-4d46-8efc-a0f2dacfb4d0",
   "metadata": {},
   "source": [
    "## Schema Independent Custom DataFrame Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e9b05455-0d6c-4df5-b61c-881b5d55ad3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_life_stage2(df, col_name):\n",
    "    return df.withColumn(\"life_stage\", life_stage(F.col(col_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "29ede03d-c92d-4da2-aea8-c46656124427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----------+\n",
      "|first_name|age|life_stage|\n",
      "+----------+---+----------+\n",
      "|     karen| 56|     adult|\n",
      "|     jodie| 16|  teenager|\n",
      "|     jason|  3|     child|\n",
      "+----------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.transform(with_life_stage2, \"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43712905-1b12-4eed-9ed6-2b6f02caade3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----------+\n",
      "|first_name|age|life_stage|\n",
      "+----------+---+----------+\n",
      "|     karen| 56|     adult|\n",
      "|     jodie| 16|  teenager|\n",
      "|     jason|  3|     child|\n",
      "+----------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.transform(with_life_stage2, col_name=\"age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952587e4-29e6-44cf-9747-c0b19abc9b3e",
   "metadata": {},
   "source": [
    "## Null best practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8a2f01ba-0b97-40c8-a245-7c245c9387f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_full_name(df):\n",
    "    return df.withColumn(\n",
    "        \"full_name\", F.concat_ws(\" \", F.col(\"first_name\"), F.col(\"last_name\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1203eddb-07fe-42c9-be49-f75a5221c806",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(\"Marilyn\", \"Monroe\"), (\"Abraham\", None), (None, None)]\n",
    ").toDF(\"first_name\", \"last_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a6b6d871-1c88-4a2b-bc12-b290d2211b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|   Marilyn|   Monroe|\n",
      "|   Abraham|     null|\n",
      "|      null|     null|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "31ba701e-4501-4801-b3c1-9faa42325ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------+\n",
      "|first_name|last_name|     full_name|\n",
      "+----------+---------+--------------+\n",
      "|   Marilyn|   Monroe|Marilyn Monroe|\n",
      "|   Abraham|     null|       Abraham|\n",
      "|      null|     null|              |\n",
      "+----------+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.transform(with_full_name).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3983647e-94d2-40de-a3b5-612cd1b721ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- full_name: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.transform(with_full_name).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3a6290-4085-44f8-b7dc-f1315165ac56",
   "metadata": {},
   "source": [
    "## Testing column function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb31e13-558c-4f2e-85be-e142bc80411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def life_stage(col):\n",
    "    return (\n",
    "        F.when(col < 13, \"child\")\n",
    "        .when(col.between(13, 19), \"teenager\")\n",
    "        .when(col > 19, \"adult\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "775c31be-9c42-4c1f-8299-6c47e1c7eebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (\"karen\", 56, \"adult\"),\n",
    "        (\"jodie\", 16, \"teenager\"),\n",
    "        (\"jason\", 3, \"child\"),\n",
    "        (None, None, None),\n",
    "    ]\n",
    ").toDF(\"first_name\", \"age\", \"expected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c7ea1fdb-d251-404e-97c5-a2dbb528a4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = df.withColumn(\"actual\", life_stage(F.col(\"age\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1ccce6a6-2b63-4459-aeb9-f21a7afe078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chispa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "976f1d25-d738-4c11-8936-25277979ab4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chispa.assert_column_equality(res, \"expected\", \"actual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238b53ab-1c17-43e7-9fe2-a795b0d11590",
   "metadata": {},
   "source": [
    "## Testing custom DataFrame transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "373af257-d618-4f23-a385-0453e193d3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_full_name(df):\n",
    "    return df.withColumn(\n",
    "        \"full_name\", F.concat_ws(\" \", F.col(\"first_name\"), F.col(\"last_name\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2dbcdcf7-f676-4775-9796-19eaeb8d75ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = spark.createDataFrame(\n",
    "    [(\"Marilyn\", \"Monroe\"), (\"Abraham\", None), (None, None)]\n",
    ").toDF(\"first_name\", \"last_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e437ea2c-8038-48ad-a5f7-13131b4d8e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_df = spark.createDataFrame(\n",
    "    [\n",
    "        (\"Marilyn\", \"Monroe\", \"Marilyn Monroe\"),\n",
    "        (\"Abraham\", None, \"Abraham\"),\n",
    "        (None, None, \"\"),\n",
    "    ]\n",
    ").toDF(\"first_name\", \"last_name\", \"full_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2e6c55ec-dbe1-47ee-88da-3c7c7ba51d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "chispa.assert_df_equality(\n",
    "    expected_df, input_df.transform(with_full_name), ignore_nullable=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c415147-8e36-4b24-9a4c-815f1db9a256",
   "metadata": {},
   "source": [
    "## PySpark UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1a600e9d-f1de-45fe-b9af-ecd149776aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries1 = spark.createDataFrame([(\"Brasil\", 1), (\"Mexico\", 2)], [\"country\", \"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5e60d1a6-2f9c-49a3-9636-71d064012d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|country| id|\n",
      "+-------+---+\n",
      "| Brasil|  1|\n",
      "| Mexico|  2|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bdc3e461-a0d7-4a65-aa14-755bab462e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def bad_funify(s):\n",
    "    return s + \" is fun!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "091a31a7-859d-4cd3-861e-757ee560a66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------------+\n",
      "|country| id|   fun_country|\n",
      "+-------+---+--------------+\n",
      "| Brasil|  1|Brasil is fun!|\n",
      "| Mexico|  2|Mexico is fun!|\n",
      "+-------+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries1.withColumn(\"fun_country\", bad_funify(\"country\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0a6dd352-c29d-4904-8f46-655db5133514",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries2 = spark.createDataFrame([(\"Thailand\", 3), (None, 4)], [\"country\", \"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "51bfa664-6cc3-43b4-9faa-76b56bbafa31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "| country| id|\n",
      "+--------+---+\n",
      "|Thailand|  3|\n",
      "|    null|  4|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a37ae762-6560-4abe-b04b-5c2d73d8cc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/18 04:47:26 ERROR Executor: Exception in task 4.0 in stage 44.0 (TID 189)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/19/_52w4zps3xjc6plz_f63j8sh0000gp/T/ipykernel_96325/680247305.py\", line 7, in bad_funify\n",
      "TypeError: unsupported operand type(s) for +: 'NoneType' and 'str'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/01/18 04:47:26 WARN TaskSetManager: Lost task 4.0 in stage 44.0 (TID 189) (192.168.0.148 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/19/_52w4zps3xjc6plz_f63j8sh0000gp/T/ipykernel_96325/680247305.py\", line 7, in bad_funify\n",
      "TypeError: unsupported operand type(s) for +: 'NoneType' and 'str'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "23/01/18 04:47:26 ERROR TaskSetManager: Task 4 in stage 44.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/var/folders/19/_52w4zps3xjc6plz_f63j8sh0000gp/T/ipykernel_96325/680247305.py\", line 7, in bad_funify\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'str'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [74]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcountries2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfun_country\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbad_funify\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcountry\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/pyspark-330-delta-210/lib/python3.9/site-packages/pyspark/sql/dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 606\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/pyspark-330-delta-210/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/pyspark-330-delta-210/lib/python3.9/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/var/folders/19/_52w4zps3xjc6plz_f63j8sh0000gp/T/ipykernel_96325/680247305.py\", line 7, in bad_funify\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'str'\n"
     ]
    }
   ],
   "source": [
    "countries2.withColumn(\"fun_country\", bad_funify(\"country\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a1e92251-c18f-4738-9650-a087ef1f75ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=StringType())\n",
    "def good_funify(s):\n",
    "    return None if s == None else s + \" is fun!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8d710d59-6dd8-402b-a92a-0be71d9d8a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------------+\n",
      "| country| id|     fun_country|\n",
      "+--------+---+----------------+\n",
      "|Thailand|  3|Thailand is fun!|\n",
      "|    null|  4|            null|\n",
      "+--------+---+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries2.withColumn(\"fun_country\", good_funify(\"country\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3221ec3f-b2a9-4b08-95a6-a071386bacb5",
   "metadata": {},
   "source": [
    "## Best to avoid a UDF completely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "07bb8bba-6ed7-48a6-bbbc-f6d9caab870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_funify(col):\n",
    "    return F.concat(col, F.lit(\" is fun!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "586e0e78-9c95-4c6a-8f59-4df9ca8be96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------------+\n",
      "| country| id|     fun_country|\n",
      "+--------+---+----------------+\n",
      "|Thailand|  3|Thailand is fun!|\n",
      "|    null|  4|            null|\n",
      "+--------+---+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries2.withColumn(\"fun_country\", best_funify(countries2.country)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1333b3b-3dce-4364-9818-ee39f55876ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark-330-delta-210] *",
   "language": "python",
   "name": "conda-env-pyspark-330-delta-210-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
